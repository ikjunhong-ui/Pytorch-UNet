{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikjunhong-ui/Pytorch-UNet/blob/master/Unet-seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAfPiJdNCFAz",
        "outputId": "75784d69-10d7-4f15-8745-d1a1601c6828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Pytorch-UNet'...\n",
            "remote: Enumerating objects: 618, done.\u001b[K\n",
            "remote: Total 618 (delta 0), reused 0 (delta 0), pack-reused 618 (from 1)\u001b[K\n",
            "Receiving objects: 100% (618/618), 47.42 MiB | 15.12 MiB/s, done.\n",
            "Resolving deltas: 100% (338/338), done.\n",
            "/content/Pytorch-UNet\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists(\"pytorch_unet.py\"):\n",
        "  if not os.path.exists(\"pytorch_unet\"):\n",
        "    # !git clone --branch import_gray_scale_image https://github.com/zgdydk123/Pytorch-UNet.git\n",
        "    !git clone https://github.com/milesial/Pytorch-UNet.git\n",
        "    %cd Pytorch-UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qBaQM7jLNXj",
        "outputId": "9d53d970-7e1f-476f-eb89-d05adadcc771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\t    evaluate.py  LICENSE     README.md\t       scripts\t unet\n",
            "Dockerfile  hubconf.py\t predict.py  requirements.txt  train.py  utils\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ufNLyxzSLfQS"
      },
      "outputs": [],
      "source": [
        "!unzip -q -j data/particle1_real.zip -d data/imgs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V6PMO185L6Si"
      },
      "outputs": [],
      "source": [
        "!unzip -q -j data/particle1_mask.zip -d data/masks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yA1Yh0jMFLv",
        "outputId": "420cf7e5-d5a4-4f6d-f0f0-1fddb14825f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mikjunhong\u001b[0m (\u001b[33mikjunhong-vanderbilt-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "!pip install -q wandb\n",
        "\n",
        "import os, wandb\n",
        "\n",
        "# replace with your actual key\n",
        "os.environ[\"WANDB_API_KEY\"] = \"9bebf141b989f4887d764ea5798df8c5cbe2ac0e\"\n",
        "\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Pytorch-UNet/data/masks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyXe-hx56Kmx",
        "outputId": "0fb30304-96fa-49a6-c267-3065652033de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Pytorch-UNet/data/masks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-F8_n_Wr6jH0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Pytorch-UNet/data/imgs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r1nCnvT77jk",
        "outputId": "2528ce75-db1f-4085-a1c5-35772f3bad70"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Pytorch-UNet/data/imgs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!a=1; for f in *.png; do mv \"$f\" \"frame${a}.png\"; a=$((a+1)); done"
      ],
      "metadata": {
        "id": "xfCU4EPG8MF_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Pytorch-UNet/data/masks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8-XZBD-6n6F",
        "outputId": "f6579b19-09d1-43cc-f6bc-e50a5ab7b600"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Pytorch-UNet/data/masks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!a=1; for f in *.png; do mv \"$f\" \"frame${a}_mask.png\"; a=$((a+1)); done"
      ],
      "metadata": {
        "id": "SHE5kdzI8tcj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Pytorch-UNet/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTU_qo2I8y3D",
        "outputId": "a67e42d4-7de2-44ba-afae-0ba8047c84a4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Pytorch-UNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i 's/UNet(\\s*n_channels\\s*=\\s*3/UNet(n_channels=1/' train.py"
      ],
      "metadata": {
        "id": "H_iDLS5N9sp8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgFNxBL2-X8J",
        "outputId": "5a35b758-362a-4155-9bd5-21e1f1a973a8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUTSiaOkMvTt",
        "outputId": "85c7c6a2-ec77-4981-e6af-264772390cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Using device cpu\n",
            "INFO: Network:\n",
            "\t1 input channels\n",
            "\t2 output channels (classes)\n",
            "\tTransposed conv upscaling\n",
            "INFO: Creating dataset with 701 examples\n",
            "INFO: Scanning mask files to determine unique values\n",
            "100% 701/701 [00:01<00:00, 366.98it/s]\n",
            "INFO: Unique mask values: [0, 255]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mikjunhong\u001b[0m (\u001b[33mikjunhong-vanderbilt-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Pytorch-UNet/wandb/run-20250902_054707-km30c7o3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearnest-totem-4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ikjunhong-vanderbilt-university/U-Net?apiKey=9bebf141b989f4887d764ea5798df8c5cbe2ac0e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ikjunhong-vanderbilt-university/U-Net/runs/km30c7o3?apiKey=9bebf141b989f4887d764ea5798df8c5cbe2ac0e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "INFO: Starting training:\n",
            "        Epochs:          100\n",
            "        Batch size:      4\n",
            "        Learning rate:   1e-06\n",
            "        Training size:   631\n",
            "        Validation size: 70\n",
            "        Checkpoints:     True\n",
            "        Device:          cpu\n",
            "        Images scaling:  1.0\n",
            "        Mixed Precision: True\n",
            "    \n",
            "/content/Pytorch-UNet/train.py:80: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Epoch 1/100:   0% 0/631 [00:00<?, ?img/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1/100:   1% 4/631 [02:32<6:37:58, 38.08s/img, loss (batch)=1.82]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Pytorch-UNet/train.py\", line 213, in <module>\n",
            "    train_model(\n",
            "  File \"/content/Pytorch-UNet/train.py\", line 101, in train_model\n",
            "    masks_pred = model(images)\n",
            "                 ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pytorch-UNet/unet/unet_model.py\", line 28, in forward\n",
            "    x3 = self.down2(x2)\n",
            "         ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pytorch-UNet/unet/unet_parts.py\", line 39, in forward\n",
            "    return self.maxpool_conv(x)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Pytorch-UNet/unet/unet_parts.py\", line 25, in forward\n",
            "    return self.double_conv(x)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python train.py --epochs 100 --scale 1.0 --batch-size 4 --learning-rat 1e-6 --amp ##new run 0826/2025"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNr2k7eSn6towmCSp+mYeQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}